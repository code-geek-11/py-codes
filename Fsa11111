from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import StructType, ArrayType

# Create Spark session
spark = SparkSession.builder.appName("FlattenJSON").getOrCreate()

# Load JSON (add limit for safe debug if needed)
df = spark.read.option("multiline", "true").json("path/to/your/file.json")

print("Original Schema:")
df.printSchema()

def flatten(df):
    """
    Recursively flattens a DataFrame with nested structs and arrays of structs.
    """
    level = 0

    while True:
        complex_fields = []
        for field in df.schema.fields:
            if isinstance(field.dataType, StructType):
                complex_fields.append((field.name, "struct"))
            elif isinstance(field.dataType, ArrayType):
                if isinstance(field.dataType.elementType, StructType):
                    complex_fields.append((field.name, "array_struct"))

        if not complex_fields:
            break

        for col_name, kind in complex_fields:
            print(f"\n[Level {level}] Processing: {col_name} ({kind})")
            if kind == "struct":
                expanded = [col(f"{col_name}.{field.name}").alias(f"{col_name}_{field.name}")
                            for field in df.schema[col_name].dataType.fields]
                df = df.select("*", *expanded).drop(col_name)

            elif kind == "array_struct":
                df = df.withColumn(col_name, explode_outer(col(col_name)))

        print(f"[Level {level}] Row count: {df.count()}")
        level += 1

    return df

# Flatten and measure time
from time import time
start = time()

flat_df = flatten(df)

print(f"\nFlattening done in {round(time() - start, 2)} seconds")

print("\nFinal Flattened Schema:")
flat_df.printSchema()

# Show only 10 rows to avoid long output
flat_df.show(10, truncate=False)
