from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode
from pyspark.sql.types import StructType, ArrayType

# Initialize Spark Session
spark = SparkSession.builder.appName("JSON to CSV").getOrCreate()

# Define the path to your JSON file
json_path = "path/to/your/nested.json"

# Read the JSON file with multiline option set to true
df = spark.read.option("multiline", "true").json(json_path)

# Debug: Print schema and a few rows to verify JSON content
print("Initial JSON Schema:")
df.printSchema()
print("Initial JSON Data:")
df.show(truncate=False)

def flatten(df):
    """
    Flatten all levels of nested structures and arrays in a DataFrame.
    """
    while True:
        # List of columns to explode (arrays)
        array_cols = [c[0] for c in df.dtypes if c[1][:5] == 'array']
        # List of columns to flatten (structs)
        struct_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']

        if not array_cols and not struct_cols:
            break

        # Explode array columns
        for col_name in array_cols:
            df = df.withColumn(col_name, explode(col(col_name)))

        # Flatten struct columns
        for col_name in struct_cols:
            for field in df.schema[col_name].dataType.fields:
                df = df.withColumn(f"{col_name}_{field.name}", col(f"{col_name}.{field.name}"))
            df = df.drop(col_name)

        # Debug: Print schema and a few rows to verify flattening steps
        print("After flattening/exploding:")
        df.printSchema()
        df.show(truncate=False)

    return df

# Flatten the DataFrame if it has nested columns
flat_df = flatten(df)

# Debug: Print the final schema and a few rows to verify final output
print("Final Flattened Schema:")
flat_df.printSchema()
print("Final Flattened Data:")
flat_df.show(truncate=False)

# Write the DataFrame to a CSV file
output_path = "path/to/output/csv"
flat_df.write.option("header", "true").csv(output_path, mode='overwrite')

# Stop the Spark Session
spark.stop()
